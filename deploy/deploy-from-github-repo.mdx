---
title: "Deploy from a Github repository"
---

Porter connects to your Github repositories to build and deploy your applications. When you connect a repository, Porter detects your app's framework, identifies services, and configures sensible defaults.

This guide covers deploying applications from Github. If you're deploying from a container registry instead, see [Deploy from a Docker registry](/deploy/deploy-from-docker-registry).

---

## Quick deploy

For most applications, Porter's automatic detection handles the heavy lifting. Here's how to get your application running:

### Connect your repository

From the Porter dashboard, navigate to your project and click Create Application. Porter attempts to deploy from a Github repository by default.

{/* [SCREENSHOT: Source selection screen showing Github and Container Registry options] */}

If this is your first time deploying from Github, you'll need to connect your Github account. Click Connect repositories to link Github, then authorize access to proceed. You can choose to grant access to all repositories or select specific ones. Porter only needs read access to detect your code, and write access to set up automated deployments.

{/* [SCREENSHOT: Github repository selector with branch dropdown] */}

Once connected, select the repository containing your application code. Porter auto-selects the repository's default branch (usually `main` or `master`) so you can start deploying immediately. If you need a different target for non‑production, use the Branch selector to switch to `dev`, `staging`, or any other branch.

### Review detected applications

After you select a repository and branch, Porter scans your code. It identifies frameworks and languages, locates Dockerfiles, and determines the required services.

{/* [SCREENSHOT: Detection in progress with loading indicator] */}

Within a few seconds, you'll see a list of detected applications. Each card shows the app name (from your repo or directory), the detected framework or build method, and the path in your repository.

{/* [SCREENSHOT: Detected applications list showing one or more application cards] */}

For a simple repository with a single application, you'll typically see one card. For monorepos containing multiple services, Porter detects each application separately. For example, a Node.js API in `/api`, a React frontend in `/web`, and a Python worker in `/jobs` each appear as distinct applications you can configure independently.

If Porter didn't detect an application you expected, or if you want to add another manually, click Add Application. To edit a detected application, click the configure button (gear icon) on the card to make updates.

{/* [SCREENSHOT: Settings button highlighted on application card] */}

### Deploy

Once applications are detected, you can review each app's start command and port, and open each app's configuration page to see other pre‑configured values. When ready, deploy using the "Deploy X applications" button.

Porter creates a Github Actions workflow in your repository that handles building and deploying your application on every push to your selected branch. Your first deployment starts as soon as you merge Porter's Github Actions pull request.

{/* [SCREENSHOT: Deployment in progress or success state] */}

That's it for a basic deployment. Porter has configured your application with production-ready defaults: appropriate resource allocation, a web service listening on the detected port, and automatic builds on every commit.

The sections below cover customizing these defaults when you need more control.

---

## Customizing your deployment

Porter's defaults work well for many applications, but you have full control over every aspect of your deployment. The following sections explain each configuration area in detail.

### Build configuration

Porter needs to know how to turn your source code into a runnable container. There are two approaches: buildpacks and Docker (via Dockerfiles).

#### Buildpacks

Buildpacks automatically detect your application's language and dependencies, then build an optimized container image without requiring you to write a Dockerfile. Porter supports buildpacks for Node.js (including Next.js), Python (Flask, FastAPI, Django), Ruby (Rails), and Go.

When Porter detects a supported framework, it selects the appropriate buildpack automatically. You'll see the detected framework displayed on the application card. If the detection isn't quite right, you can override it by selecting a different framework from the dropdown in an application's configuration page.

{/* [SCREENSHOT: Build method selector showing framework options] */}

Buildpacks work well when your application follows standard conventions for its framework. They handle dependency installation, asset compilation, and runtime configuration automatically.

#### Docker builds

If your repository contains a Dockerfile, Porter can use it to build your application instead of buildpacks. This gives you complete control over the build process and is the right choice when you have custom system dependencies, need a specific base image, or have an application that buildpacks don't support.

When you select Docker as your build method, you'll need to specify the path to your Dockerfile relative to the repository root. If your Dockerfile is at the root level, this is simply `Dockerfile`. For monorepos, it might be `./api/Dockerfile` or `./services/web/Dockerfile`.

{/* [SCREENSHOT: Dockerfile path selector] */}

#### Build context

The build context determines which directory Porter uses as the root for your build. For most repositories, this is `./` (the repository root). In a monorepo, you'll typically set this to the directory containing the specific application, like `./api` or `./frontend`.

The build context affects where Porter looks for your Dockerfile (if using Docker builds) and which files are available during the build process.

{/* [SCREENSHOT: Build context path selector] */}

---

### Understanding services

A Porter application consists of one or more **services**. Services are the individual processes that make up your application, and they come in three types:

**Web services** handle HTTP traffic. If your application serves a website, API, or any other HTTP endpoint, it runs as a web service. Web services can be exposed to the internet with custom domains, or kept private within your cluster for internal communication.

**Worker services** run continuously in the background without accepting HTTP traffic. Use workers for queue processors, background job runners, event consumers, or any long-running process that doesn't need to respond to web requests.

**Job services** run on a schedule or on-demand, execute their task, and then stop. Use jobs for database maintenance, report generation, cleanup tasks, or any work that runs periodically rather than continuously.

{/* [SCREENSHOT: Service type selector dropdown showing Web, Worker, and Job options] */}

By default, Porter configures a single web service for detected applications. You can add additional services by clicking **Add Service** and selecting the appropriate type. Each service within an application shares the same codebase and build, but can have different start commands, resource allocations, and configurations.

Service names must be lowercase letters, numbers, and hyphens only. They're used internally for routing and identification.

---

### Resource allocation and scaling

Every service needs compute resources. Porter lets you configure exactly how much CPU and memory each service receives, and how it scales under load.

#### CPU and memory

CPU is measured in cores, configurable from 0.1 (one-tenth of a core) up to 8 cores. Memory is measured in megabytes, from 128 MB up to 16 GB. The defaults (0.5 cores and 1 GB of memory) work well for lightweight services. Increase these values for compute-intensive workloads or applications with large memory footprints.

{/* [SCREENSHOT: Resource allocation sliders for CPU and RAM] */}

These values represent guaranteed resources. Your service will always have access to at least this much CPU and memory, regardless of what else is running in the cluster.

#### Node groups

If your cluster has more than one node group to pick from, you can select them here. When you choose a node group with GPU support, an additional slider appears for configuring GPU allocation.

For most applications, the default node group is appropriate.

#### Instance count and autoscaling

By default, Porter runs a single instance of each service. For production workloads, you'll typically want multiple instances for redundancy and to handle traffic spikes.

Porter offers three scaling modes:

**Fixed instances** runs a constant number of replicas. Set this to 2 or more for high availability. If one instance fails, others continue serving traffic while a replacement starts.

**Resource-based autoscaling** automatically adjusts the number of instances based on CPU and memory utilization. You configure a minimum and maximum instance count, along with target utilization percentages.

{/* [SCREENSHOT: Autoscaling configuration with resource-based mode selected] */}

**Custom autoscaling** uses [KEDA (Kubernetes Event-Driven Autoscaling)](https://keda.sh) to scale based on any Prometheus metric. This is useful for scaling workers based on queue depth, or web services based on request latency. You provide a PromQL query and a threshold value—Porter scales up when the metric exceeds the threshold and scales down when it drops below.

{/* [SCREENSHOT: Custom autoscaling with KEDA metric configuration] */}


---

### Networking and domains

Web services can be exposed to the internet or kept private within your cluster.

#### Public vs. private

By default, web services are public—accessible from the internet. Toggle a service to **private** when it should only be reachable by other services in your cluster. Private services are useful for internal APIs, admin interfaces, or services that sit behind a public-facing gateway.

{/* [SCREENSHOT: Networking tab showing public/private toggle] */}

#### Custom domains

To serve your application on your own domain, add it in the Domains section. You can configure multiple domains for a single service. This is useful for handling `www` and non-`www` versions, or serving the same application on different domains.

{/* [SCREENSHOT: Custom domain configuration with domain input field] */}

After adding a domain, you'll need to configure DNS. Porter displays your cluster's ingress IP address with a copy button. Create a CNAME record (or an A record for apex domains) pointing your domain to this address. DNS propagation typically takes a few minutes, though it can occasionally take longer.

{/* [SCREENSHOT: DNS configuration instructions showing cluster ingress IP] */}

Porter automatically provisions and renews SSL certificates for your custom domains using Let's Encrypt.

#### Advanced routing with NGINX annotations

For complex routing scenarios, you can add custom NGINX ingress annotations. These key-value pairs are applied directly to the Kubernetes ingress resource, giving you access to NGINX's full feature set.

Common uses include custom rewrite rules, rate limiting, authentication requirements, CORS headers, and proxy buffer configuration. The annotation keys follow the `nginx.ingress.kubernetes.io/` prefix convention.

{/* [SCREENSHOT: Custom NGINX annotations key-value editor] */}

---

### Environment variables and secrets

Most applications need configuration values that vary between environments: database URLs, API keys, feature flags, and other settings.

#### Adding variables

On the app configuration page, expand the **Environment variables** accordion to define key-value pairs that become environment variables in your running containers. Type the variable name and value, and click the lock icon to mark a value as a secret.

{/* [SCREENSHOT: Environment variable key-value editor with lock icons] */}

The distinction between variables and secrets affects visibility in the Porter dashboard. Secret values are masked and can only be revealed intentionally. Both are stored securely and injected into your containers at runtime.

#### Environment groups

If you have variables shared across multiple applications (like a database connection string or third-party API key) you can organize them into environment groups. Select existing groups from the dropdown to sync their variables into your application.

{/* [SCREENSHOT: Environment groups selector showing available groups] */}

When you update a variable in an environment group, the change propagates to all applications using that group on their next deployment.

#### Uploading .env files

For applications with many environment variables, you can upload an existing `.env` file rather than entering each variable manually. Click **Upload an .env file** and paste in your file contents. Porter parses the `KEY=VALUE` format, skipping comments and empty lines.

{/* [SCREENSHOT: .env file upload modal] */}

---

### Health checks and observability

#### Health checks

Health checks enable zero-downtime deployments by ensuring new instances are ready before receiving traffic. When enabled, Porter waits for your health endpoint to return a successful response before routing traffic to a new instance, and automatically restarts instances that become unhealthy.

{/* [SCREENSHOT: Health check configuration with endpoint and timing options] */}

Configure the health check with an HTTP path (like `/health` or `/api/health`) that your application exposes. The endpoint should return a 200-level status code when the service is ready to handle requests.

The **initial delay** gives your application time to start up before health checks begin. Set this higher if your application has a slow initialization process.

The **timeout** determines how long Porter waits for a response before considering the check failed.

#### Metrics scraping

If your application exposes Prometheus metrics, Porter can scrape and forward them to your monitoring infrastructure. Enable metrics scraping and specify the port and path where your application serves metrics (commonly `/metrics` on the application port or a dedicated metrics port).

{/* [SCREENSHOT: Metrics scraping configuration] */}

#### Sleep mode

For non-production environments, sleep mode lets you pause a service to save costs. Sleeping services maintain their configuration but stop running instances. This is useful for staging environments that don't need to run overnight or on weekends.

---

### Scheduled jobs and background workers

Not every workload serves HTTP traffic. Porter supports background workers and scheduled jobs for processing that happens outside the request-response cycle.

#### Worker services

Workers run continuously, processing tasks from queues, handling events, or performing ongoing background work. Configure a worker with a start command that runs your processing logic: for example, `python worker.py` or `node src/consumer.js`.

{/* [SCREENSHOT: Worker service configuration showing start command and resources] */}

Workers support the same resource allocation and autoscaling options as web services. For queue-based workers, custom autoscaling with KEDA lets you scale based on queue depth, adding workers when messages back up and removing them when the queue empties.

#### Job services

Jobs run on a schedule and exit when complete. The cron schedule field accepts standard cron syntax with five fields: minute, hour, day of month, month, and day of week. Porter displays a human-readable description of your schedule as you type.

{/* [SCREENSHOT: Job configuration with cron schedule showing human-readable description] */}

Some example schedules:
- `0 0 * * *` runs daily at midnight
- `0 */4 * * *` runs every 4 hours
- `0 9 * * 1-5` runs at 9 AM on weekdays
- `*/15 * * * *` runs every 15 minutes

Configure the timeout to set a maximum execution time. Jobs exceeding this limit are terminated, preventing runaway processes from consuming resources indefinitely.

The **concurrent execution** toggle controls whether multiple instances of the same job can run simultaneously. Disable this for jobs that shouldn't overlap.

**Suspend cron job** temporarily pauses the schedule without removing the job configuration, useful during maintenance windows.

---

### Pre-deployment jobs

Some deployments need setup work before the main application starts, most commonly, database migrations. Pre-deployment jobs (also called migration jobs) run after your new code builds but before traffic routes to new instances.

{/* [SCREENSHOT: Pre-deployment job configuration panel] */}

Enable the pre-deployment job and configure a start command for your migration script: for example, `npm run migrate`, `python manage.py migrate`, or `bundle exec rails db:migrate`.

Pre-deployment jobs have their own resource allocation separate from your application services. Migrations are typically short-lived but may need more memory than your running application, especially for large data transformations.

Configure an appropriate timeout based on how long your migrations typically take. The job must complete successfully before deployment continues. If it fails, the deployment halts, and your previous version continues running—your users never see a partially-migrated state.

---

## What happens when you deploy

When you click Deploy, Porter initiates several processes:

First, Porter creates a Github Actions workflow file in your repository at `.github/workflows/porter.yml`. This workflow triggers on pushes to your selected branch, building your application and deploying it to Porter. You'll see a pull request created with this workflow file, and merging this PR enables automated deployments.

The initial deployment builds your application image using the configured build method and starts your services with the specified configuration. You can monitor deployment progress from the application dashboard, which shows build logs, deployment status, and any errors that occur.

{/* [SCREENSHOT: Deployment progress view or application dashboard] */}

Once deployed, subsequent pushes to your branch trigger automatic rebuilds and deployments. Each deployment creates new container instances, runs health checks (if configured), and shifts traffic only after the new instances are ready.

From your application dashboard, you can view logs, monitor resource usage, check deployment history, and make configuration changes. Changes to configuration trigger new deployments, while changes to environment variables can often take effect without a full rebuild.
